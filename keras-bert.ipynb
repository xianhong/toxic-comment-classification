{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["### Install packages\n", "* Hamming loss metrc in tensorflow addon seems to require Tf. 2.2 otherwise it may not work."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install --upgrade tensorflow==2.2.0-rc2\n", "!pip install bert-for-tf2\n", "!pip install tensorflow-addons\n", "!unzip /kaggle/input/jigsaw-toxic-comment-classification-challenge/train.csv.zip\n", "!unzip /kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import tensorflow as tf\n", "import tensorflow_hub as hub\n", "import bert\n", "from tensorflow.keras.models import  Model\n", "from tqdm import tqdm\n", "import numpy as np\n", "from collections import namedtuple\n", "from imblearn.over_sampling import RandomOverSampler\n", "import pandas as pd\n", "from sklearn.model_selection import train_test_split\n", "from collections import Counter\n", "# multi-label hamming loss\n", "import tensorflow_addons as tfa\n", "hl = tfa.metrics.HammingLoss(mode='multilabel', threshold=0.4)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Download BERT base model (fix model parameters!)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["bert_layer=hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=False)\n", "MAX_SEQ_LEN=128\n", "input_word_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,\n", "                                       name=\"input_word_ids\")\n", "input_mask = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,\n", "                                   name=\"input_mask\")\n", "segment_ids = tf.keras.layers.Input(shape=(MAX_SEQ_LEN,), dtype=tf.int32,\n", "                                    name=\"segment_ids\")\n", "pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Add a multi-label classifier on top of BERT \n", "* use hamming loss and custom defined positive labels recall as metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["@tf.function\n", "def custom_recall(y_true,y_pred):\n", "    y_true = tf.convert_to_tensor(y_true)\n", "    Y_PRED = tf.cast(y_pred>=0.4,tf.float32)\n", "    recall = tf.math.divide_no_nan(tf.math.reduce_sum(tf.math.multiply(y_true,Y_PRED)),tf.math.reduce_sum(y_true))\n", "    return recall\n", "x = tf.keras.layers.Dropout(0.1)(pooled_output)\n", "x= tf.keras.layers.Dense(128, activation='relu')(x)\n", "x = tf.keras.layers.Dropout(0.1)(x)\n", "out = tf.keras.layers.Dense(6, activation=\"sigmoid\", name=\"dense_output\")(x)\n", "model = tf.keras.models.Model(\n", "      inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n", "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[hl,custom_recall])\n", "model.summary()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["FullTokenizer=bert.bert_tokenization.FullTokenizer\n", "vocab_file=bert_layer.resolved_object.vocab_file.asset_path.numpy()\n", "do_lower_case=bert_layer.resolved_object.do_lower_case.numpy()\n", "tokenizer=FullTokenizer(vocab_file,do_lower_case)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Print out RECALL metric ,Hamming loss and Confusion matrix for the results.\n", "#\n", "def results(pred_results,ground_truth,threshold):\n", "    hl = tfa.metrics.HammingLoss(mode='multilabel', threshold=threshold)\n", "    hl.update_state(ground_truth, pred_results)\n", "    print('Hamming loss:', hl.result().numpy())\n", "    pred_results = (pred_results>=threshold).astype(int)\n", "    print(\"Positive labels RECALL-(thre:{}):  {}%\".format(threshold,100*np.sum(pred_results*ground_truth)/np.sum(ground_truth)))\n", "    cf = tfa.metrics.MultiLabelConfusionMatrix(num_classes=6)\n", "    cf.update_state(ground_truth, pred_results)\n", "    print('Confusion matrix:', cf.result().numpy())\n", "    return \n", "\n", "# Resampling of training set based on the 'k'th label (k=0,1,..5)\n", "#   Return: Resampled set of 100K samples.\n", "#\n", "def RS(k,features_train,ratio=1.0):\n", "    train_y = features_train[:,3*128:390]\n", "    label = train_y[:,k]\n", "    rus = RandomOverSampler(ratio,random_state=42)\n", "    features_train_res, _ = rus.fit_resample(features_train, label)\n", "    np.random.shuffle(features_train_res)\n", "    features_train_res = features_train_res[:100000,:]\n", "    return features_train_res\n", "\n", "def index_set(k,pattern):\n", "    m= 2**(5-k)\n", "    return set([i for i in range(64) if (i//m%2)!=0]).intersection(set(pattern.keys()))\n", "\n", "def label_to_class(v):\n", "    w = np.array([32,16,8,4,2,1],dtype=np.int16)\n", "    return np.sum(v*w,axis=1)\n", "\n", "def class_to_label(c):\n", "    c=np.reshape(c,(-1,1))\n", "    v5=c%2\n", "    c=c//2\n", "    v4=c%2\n", "    c=c//2\n", "    v3=c%2\n", "    c=c//2\n", "    v2=c%2\n", "    c=c//2\n", "    v1=c%2\n", "    c=c//2\n", "    v0=c%2\n", "    return np.concatenate([v0,v1,v2,v3,v4,v5],axis=1)\n", "def distribution(pattern):\n", "    b=class_to_label(np.array(list(pattern.keys())))\n", "    a=np.asarray(list(pattern.values())).reshape(-1,1)\n", "    M = a*b\n", "    w_vec=np.sum(M,axis=0)\n", "    #return w_vec/np.sum(w_vec)\n", "    return w_vec\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Functions to process data to be fed into BERT"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_masks(tokens, max_seq_length):\n", "    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n", "def get_segments(tokens, max_seq_length):\n", "    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n", "    segments = []\n", "    current_segment_id = 0\n", "    for token in tokens:\n", "        segments.append(current_segment_id)\n", "        if token == \"[SEP]\":\n", "            current_segment_id = 1\n", "    return segments + [0] * (max_seq_length - len(tokens))\n", "def get_ids(tokens, tokenizer, max_seq_length):\n", "    \"\"\"Token ids from Tokenizer vocab\"\"\"\n", "    token_ids = tokenizer.convert_tokens_to_ids(tokens,)\n", "    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n", "    return input_ids\n", "\n", "def create_single_input(sentence,MAX_LEN):\n", "    stokens = tokenizer.tokenize(sentence)\n", "    stokens = stokens[:MAX_LEN]\n", "    stokens = [\"[CLS]\"] + stokens + [\"[SEP]\"]\n", "    ids = get_ids(stokens, tokenizer, MAX_SEQ_LEN)\n", "    masks = get_masks(stokens, MAX_SEQ_LEN)\n", "    segments = get_segments(stokens, MAX_SEQ_LEN)\n", "    return ids,masks,segments\n", "def create_input_array(sentences):\n", "    input_ids, input_masks, input_segments = [], [], []\n", "    for sentence in tqdm(sentences,position=0, leave=True):\n", "        ids,masks,segments=create_single_input(sentence,MAX_SEQ_LEN-2)\n", "        input_ids.append(ids)\n", "        input_masks.append(masks)\n", "        input_segments.append(segments)\n", "    return [np.asarray(input_ids, dtype=np.int32), \n", "            np.asarray(input_masks, dtype=np.int32), \n", "            np.asarray(input_segments, dtype=np.int32)]\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Prepare training & test data\n", "* To address extreme imbalance nature of the dataset, use multiple rounds of resampling to increase the proportion of minority class samples in training set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df=pd.read_csv('/kaggle/working/train.csv')\n", "df = df.sample(frac=1)\n", "train_sentences = df[\"comment_text\"].fillna(\"CVxTz\").values\n", "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n", "inputs=create_input_array(train_sentences)\n", "\n", "train_y = df[list_classes].values\n", "features=np.concatenate(inputs,axis=1)\n", "features=np.concatenate((features,train_y),axis=1)\n", "\n", "features_train,features_test,train_y,_= train_test_split(features, train_y,test_size=0.2,random_state=42)\n", "print(\"Train set samples:\",train_y.shape[0])\n", "\n", "print(\"Positive labels distribution:\",distribution(Counter(label_to_class(train_y)))/train_y.shape[0])\n", "\n", "prior = distribution(Counter(label_to_class(train_y)))/train_y.shape[0]\n", "list_trains = []\n", "features_train_res= RS(3,features_train)\n", "list_trains.append(features_train_res)\n", "features_train_res= RS(5,features_train)\n", "list_trains.append(features_train_res)\n", "features_train_res= RS(1,features_train)\n", "list_trains.append(features_train_res)\n", "features_train_res= RS(4,features_train)\n", "list_trains.append(features_train_res)\n", "features_train_res= RS(2,features_train)\n", "list_trains.append(features_train_res)\n", "features_train_res= RS(0,features_train)\n", "list_trains.append(features_train_res)\n", "\n", "# Concatenate six rounds of resampling,each producing 100K samples, making a total of 600K sample training set\n", "features_train=np.concatenate(list_trains,axis=0)\n", "np.random.shuffle(features_train)\n", "train_y = features_train[:,3*128:390]\n", "after = distribution(Counter(label_to_class(train_y)))/600000\n", "print(\"Positive sample amplification through resampling:\",after/prior)\n", "print(\"Positive labels distribution after resampling:\",after)\n", "\n", "X = np.split(features_train,[128,2*128,3*128,390],axis=1)\n", "input_ids=X[0]\n", "input_masks=X[1]\n", "input_segments=X[2]\n", "train_y= X[3].astype(np.float32)\n", "\n", "X = np.split(features_test,[128,2*128,3*128,390],axis=1)\n", "input_ids_test=X[0]\n", "input_masks_test=X[1]\n", "input_segments_test=X[2]\n", "test_y= X[3].astype(np.float32)\n", "\n", "inputs_train=[input_ids,input_masks,input_segments]\n", "inputs_test=[input_ids_test,input_masks_test,input_segments_test]\n", "print(\"Test set samples:\",test_y.shape[0])\n", "print(\"Train set samples after resampling:\",train_y.shape[0])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model.fit(inputs_train,train_y,epochs=2,batch_size=64,shuffle=True)\n", "model.save_weights('/kaggle/working/chkpt')\n", "#model.load_weights('/kaggle/input/keras-bert/chkpt')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["pred_results = model.predict(inputs_test,batch_size=256)\n", "ground_truth = test_y.astype(int)\n", "results(pred_results,ground_truth,0.4)\n", "results(pred_results,ground_truth,0.3)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!rm /kaggle/working/*.csv"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.6"}}, "nbformat": 4, "nbformat_minor": 4}